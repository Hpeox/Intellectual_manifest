{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "import cv2, time\n",
    "# from PIL import Image\n",
    "# from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "import torchmetrics\n",
    "import torchvision.models as models\n",
    "# import torchvision.transforms as transforms\n",
    "# import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3070 Ti Laptop GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "train_path = glob.glob(\"./data/sample/*/*\")\n",
    "test_path = glob.glob(\"./data/test/*\")\n",
    "\n",
    "train_path.sort()\n",
    "test_path.sort()\n",
    "\n",
    "# train_df = pd.read_csv(\"data/train.csv\")\n",
    "# train_df = train_df.sort_values(by=\"name\")\n",
    "# train_label = train_df[\"label\"].values\n",
    "\n",
    "train_label = [np.int64(0) for i in range(480)]+[np.int64(1) for i in range(2400)]\n",
    "\n",
    "# 自定义数据集\n",
    "# 带有图片缓存的逻辑\n",
    "DATA_CACHE = {}\n",
    "\n",
    "\n",
    "class XunFeiDataset(Dataset):\n",
    "    def __init__(self, img_path, img_label, transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.img_label = img_label\n",
    "        if transform is not None:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.img_path[index] in DATA_CACHE:\n",
    "            img = DATA_CACHE[self.img_path[index]]\n",
    "        else:\n",
    "            img = cv2.imread(self.img_path[index])\n",
    "            DATA_CACHE[self.img_path[index]] = img\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(image=img)[\"image\"]\n",
    "        img = img.transpose([2, 0, 1])\n",
    "        return img, torch.from_numpy(np.array(self.img_label[index]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "dataset = XunFeiDataset(\n",
    "    train_path,\n",
    "    train_label,\n",
    "    A.Compose(\n",
    "        [\n",
    "            A.RandomRotate90(),\n",
    "            A.Resize(256, 256),\n",
    "            A.RandomCrop(224, 224),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset=dataset, lengths=[0.95, 0.05], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# 训练集\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=False\n",
    ")\n",
    "\n",
    "# 验证集\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=False\n",
    ")\n",
    "\n",
    "# 测试集\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    XunFeiDataset(\n",
    "        test_path,\n",
    "        [0] * len(test_path),\n",
    "        A.Compose(\n",
    "            [\n",
    "                A.Resize(256, 256),\n",
    "                A.RandomCrop(224, 224),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.5),\n",
    "                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=50,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 112, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=cv2.imread(train_path[0])\n",
    "np.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XunFeiNet50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XunFeiNet50, self).__init__()\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        model.fc = nn.Linear(2048, 2)\n",
    "        self.resnet = model\n",
    "    def forward(self, img):\n",
    "        out = self.resnet(img)\n",
    "        return out\n",
    "        \n",
    "class XunFeiNet101(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XunFeiNet101, self).__init__()\n",
    "        model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "        model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        model.fc = nn.Linear(2048, 2)\n",
    "        self.resnet = model\n",
    "    def forward(self, img):\n",
    "        out = self.resnet(img)\n",
    "        return out\n",
    "    \n",
    "class XunFeiNet152(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XunFeiNet152, self).__init__()\n",
    "        model = models.resnet152(weights=models.ResNet152_Weights.DEFAULT)\n",
    "        model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        model.fc = nn.Linear(2048, 2)\n",
    "        self.resnet = model\n",
    "    def forward(self, img):\n",
    "        out = self.resnet(img)\n",
    "        return out\n",
    "    \n",
    "class XunFeiNet34(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XunFeiNet34, self).__init__()\n",
    "        model = models.resnet34()\n",
    "        model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        model.fc = nn.Linear(512, 2)\n",
    "        self.resnet = model\n",
    "    def forward(self, img):\n",
    "        out = self.resnet(img)\n",
    "        return out\n",
    "    \n",
    "model34 = XunFeiNet34().to(device)\n",
    "model50 = XunFeiNet50().to(device)\n",
    "model101= XunFeiNet101().to(device)\n",
    "model152= XunFeiNet152().to(device)\n",
    "# model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train(train_loader, model, criterion, optimizer):\n",
    "    start = time.time()\n",
    "    start_batch = [start, 0]\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    preds = torch.tensor([])\n",
    "    target_all = torch.tensor([])\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            start_batch[(1+(i + 1) // 100) % 2] = time.time()\n",
    "            print(\n",
    "                \"Train loss\",\n",
    "                loss.item(),\n",
    "                \"t={}s\".format(\n",
    "                    start_batch[(1+(i + 1) // 100) % 2]\n",
    "                    - start_batch[((i + 1) // 100) % 2]\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        preds = torch.cat((preds, output.cpu().argmax(1)))\n",
    "        target_all = torch.cat((target_all, target.cpu()))\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    val_acc = torchmetrics.functional.classification.multiclass_f1_score(\n",
    "        preds, target_all, num_classes=2, average=\"macro\"\n",
    "    )\n",
    "    print(\"t={}s\".format(time.time() - start))\n",
    "    print(\"F1 score\", val_acc)\n",
    "    return train_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# 模型验证\n",
    "def validate(val_loader, model, criterion):\n",
    "    model.eval()\n",
    "    val_acc = 0.0\n",
    "    preds = torch.tensor([])\n",
    "    target_all = torch.tensor([])\n",
    "    val_loss=0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            # val_acc += (output.argmax(1) == target).sum().item()\n",
    "            preds = torch.cat((preds, output.cpu().argmax(1)))\n",
    "            target_all = torch.cat((target_all, target.cpu()))\n",
    "\n",
    "        val_acc = torchmetrics.functional.classification.multiclass_f1_score(\n",
    "            preds, target_all, num_classes=2, average=\"macro\"\n",
    "        )\n",
    "        val_loss += loss.item()\n",
    "    # return val_acc / len(val_loader.dataset)\n",
    "    return val_acc,val_loss/len(val_loader)\n",
    "\n",
    "\n",
    "# 模型预测\n",
    "def predict(test_loader, model):\n",
    "    model.eval()\n",
    "\n",
    "    test_pred = []\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(test_loader):\n",
    "            input = input.to(device)\n",
    "            output = model(input)\n",
    "            test_pred.append(output.data.cpu().numpy())\n",
    "\n",
    "    return np.vstack(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for best model\n",
    "model=model152\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.000005)\n",
    "# model=model101\n",
    "epochs = 0\n",
    "last_acc = 0\n",
    "val_acc = 0.001\n",
    "while last_acc < val_acc:\n",
    "    last_acc = val_acc\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    # train_acc = validate(train_loader, model)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epochs + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )\n",
    "    epochs += 1\n",
    "    if val_acc>last_acc:\n",
    "        torch.save(model.state_dict(),\"./model/baseline_152.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.59358149766922 t=37.89586925506592s\n",
      "Train loss 0.5492005944252014 t=65.1143262386322s\n",
      "Train loss 0.5439359545707703 t=89.79940986633301s\n",
      "Train loss 0.498115599155426 t=114.49705696105957s\n",
      "Train loss 0.5084092020988464 t=139.29148244857788s\n",
      "Train loss 0.535024881362915 t=164.13810873031616s\n",
      "Train loss 0.4228997528553009 t=189.23814034461975s\n",
      "Train loss 0.39884406328201294 t=215.17427492141724s\n",
      "t=233.4092059135437s\n",
      "F1 score tensor(0.4689)\n",
      "epoch 1 : train_loss: 0.5194392675577209 val_loss: 0.10963937044143676 val_f1: tensor(0.4504)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ./model does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m epochs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[39mif\u001b[39;00m val_acc\u001b[39m>\u001b[39mlast_acc:\n\u001b[1;32m---> 18\u001b[0m     torch\u001b[39m.\u001b[39;49msave(model\u001b[39m.\u001b[39;49mstate_dict(),\u001b[39m\"\u001b[39;49m\u001b[39m./model/baseline_101.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32md:\\programs\\Anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 440\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32md:\\programs\\Anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[1;32md:\\programs\\Anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mstr\u001b[39;49m(name)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory ./model does not exist."
     ]
    }
   ],
   "source": [
    "# for best model\n",
    "model=model101\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.000005)\n",
    "# model=model101\n",
    "epochs = 0\n",
    "last_acc = 0\n",
    "val_acc = 0.001\n",
    "while last_acc < val_acc:\n",
    "    last_acc = val_acc\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    # train_acc = validate(train_loader, model)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epochs + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )\n",
    "    epochs += 1\n",
    "    if val_acc>last_acc:\n",
    "        torch.save(model.state_dict(),\"./model/baseline_101.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.5575281381607056 t=4.891497373580933s\n",
      "Train loss 0.5705251097679138 t=10.128498792648315s\n"
     ]
    }
   ],
   "source": [
    "# for best model\n",
    "model=model50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.000005)\n",
    "# model=model101\n",
    "epochs = 0\n",
    "last_acc = 0\n",
    "val_acc = 0.001\n",
    "while last_acc < val_acc:\n",
    "    last_acc = val_acc\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    # train_acc = validate(train_loader, model)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epochs + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )\n",
    "    epochs += 1\n",
    "    if val_acc>last_acc:\n",
    "        torch.save(model.state_dict(),\"./model/baseline_50.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for best model\n",
    "model=model34\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.000005)\n",
    "# model=model101\n",
    "epochs = 0\n",
    "last_acc = 0\n",
    "val_acc = 0.001\n",
    "while last_acc < val_acc:\n",
    "    last_acc = val_acc\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    # train_acc = validate(train_loader, model)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epochs + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )\n",
    "    epochs += 1\n",
    "    if val_acc>last_acc:\n",
    "        torch.save(model.state_dict(),\"./model/baseline_34.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.027153223752975464 t=-2.7770018577575684s\n",
      "Train loss 0.24580799043178558 t=-5.874910831451416s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m----> 5\u001b[0m     train_loss \u001b[39m=\u001b[39m train(train_loader, model, criterion, optimizer)\n\u001b[0;32m      6\u001b[0m     val_acc,val_loss \u001b[39m=\u001b[39m validate(val_loader, model, criterion)\n\u001b[0;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m      8\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m :\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mtrain_loss:\u001b[39m\u001b[39m\"\u001b[39m, train_loss, \u001b[39m'\u001b[39m\u001b[39mval_loss:\u001b[39m\u001b[39m'\u001b[39m, val_loss,\u001b[39m\"\u001b[39m\u001b[39mval_f1:\u001b[39m\u001b[39m\"\u001b[39m, val_acc\n\u001b[0;32m      9\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[23], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, criterion, optimizer)\u001b[0m\n\u001b[0;32m     19\u001b[0m     start_batch[(\u001b[39m1\u001b[39m\u001b[39m+\u001b[39m(i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m100\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m     21\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrain loss\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     22\u001b[0m         loss\u001b[39m.\u001b[39mitem(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m         ),\n\u001b[0;32m     27\u001b[0m     )\n\u001b[1;32m---> 29\u001b[0m preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((preds, output\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39margmax(\u001b[39m1\u001b[39m)))\n\u001b[0;32m     30\u001b[0m target_all \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((target_all, target\u001b[39m.\u001b[39mcpu()))\n\u001b[0;32m     32\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model=model34\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.000005)\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epoch + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对测试集多次预测\n",
    "pred = None\n",
    "model=model101\n",
    "model.load_state_dict(torch.load(\"./model/baseline_101.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += predict(test_loader, model)\n",
    "    print(_+1)\n",
    "model=model152\n",
    "model.load_state_dict(torch.load(\"./model/baseline_152.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += 0.9*predict(test_loader, model)\n",
    "    print(_+1)\n",
    "model=model50\n",
    "model.load_state_dict(torch.load(\"./model/baseline_50.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += predict(test_loader, model)\n",
    "    print(_+1)\n",
    "model=model34\n",
    "model.load_state_dict(torch.load(\"./model/baseline_34.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += 0.9*predict(test_loader, model)\n",
    "    print(_+1)\n",
    "submit = pd.DataFrame(\n",
    "    {\n",
    "        'name': [x.split('/')[-1] for x in test_path],\n",
    "        'label': pred.argmax(1)\n",
    "})\n",
    "\n",
    "# 生成提交结果\n",
    "submit = submit.sort_values(by='name')\n",
    "submit.to_csv('submit6.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_loss = 0.0\n",
    "model = model.to(device)\n",
    "for i, (input, target) in enumerate(train_loader):\n",
    "    input = input.to(device)\n",
    "    target = target.to(device)\n",
    "    output = model(input)\n",
    "    loss = criterion(output, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=4\n",
    "model=model101\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.0001)\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epoch + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )\n",
    "    epochs += 1\n",
    "torch.save(model.state_dict(),\"./model/baseline_test.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=3\n",
    "model101= XunFeiNet().to(device)\n",
    "model=model101\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.0001)\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epoch + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )\n",
    "    epochs += 1\n",
    "torch.save(model.state_dict(),\"./model/baseline_101.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = None\n",
    "model=model101\n",
    "model.load_state_dict(torch.load(\"./model/baseline_test.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += predict(test_loader, model)\n",
    "    print(_+1)\n",
    "model.load_state_dict(torch.load(\"./model/baseline_101.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += predict(test_loader, model)\n",
    "    print(_+1)\n",
    "model=model50\n",
    "model.load_state_dict(torch.load(\"./model/baseline_50.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += predict(test_loader, model)\n",
    "    print(_+1)\n",
    "submit = pd.DataFrame(\n",
    "    {\n",
    "        'name': [x.split('/')[-1] for x in test_path],\n",
    "        'label': pred.argmax(1)\n",
    "})\n",
    "\n",
    "# 生成提交结果\n",
    "submit = submit.sort_values(by='name')\n",
    "submit.to_csv('submit5.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = validate(val_loader, model)\n",
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=XunFeiDataset(train_path[:-1000], train_label[:-1000],\n",
    "            A.Compose([\n",
    "            # A.RandomRotate90(),\n",
    "            A.Resize(256, 256),\n",
    "            A.RandomCrop(224, 224),\n",
    "            # A.HorizontalFlip(p=0.5),\n",
    "            # A.RandomBrightnessContrast(p=0.5),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        )\n",
    "train_dataset, test_dataset=torch.utils.data.random_split(dataset=dataset,lengths=[0.9,0.1],generator=torch.Generator().manual_seed(42))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./model/baseline_test.pth\"))\n",
    "val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "print(val_loss)\n",
    "len(val_loader.dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
