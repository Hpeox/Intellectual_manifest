{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "import cv2, time\n",
    "# from PIL import Image\n",
    "# from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "import torchmetrics\n",
    "import torchvision.models as models\n",
    "# import torchvision.transforms as transforms\n",
    "# import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3070 Ti Laptop GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "train_path = glob.glob(\"./data/sample/*/*\")\n",
    "test_path = glob.glob(\"./data/test/*\")\n",
    "\n",
    "train_path.sort()\n",
    "test_path.sort()\n",
    "\n",
    "# train_df = pd.read_csv(\"data/train.csv\")\n",
    "# train_df = train_df.sort_values(by=\"name\")\n",
    "# train_label = train_df[\"label\"].values\n",
    "\n",
    "train_label = [np.int64(0) for i in range(480)]+[np.int64(1) for i in range(2400)]\n",
    "\n",
    "# 自定义数据集\n",
    "# 带有图片缓存的逻辑\n",
    "DATA_CACHE = {}\n",
    "\n",
    "\n",
    "class XunFeiDataset(Dataset):\n",
    "    def __init__(self, img_path, img_label, transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.img_label = img_label\n",
    "        if transform is not None:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.img_path[index] in DATA_CACHE:\n",
    "            img = DATA_CACHE[self.img_path[index]]\n",
    "        else:\n",
    "            img = cv2.imread(self.img_path[index])\n",
    "            DATA_CACHE[self.img_path[index]] = img\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(image=img)[\"image\"]\n",
    "        img = img.transpose([2, 0, 1])\n",
    "        return img, torch.from_numpy(np.array(self.img_label[index]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "dataset = XunFeiDataset(\n",
    "    train_path,\n",
    "    train_label,\n",
    "    A.Compose(\n",
    "        [\n",
    "            A.RandomRotate90(),\n",
    "            A.Resize(256, 256),\n",
    "            A.RandomCrop(224, 224),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset=dataset, lengths=[0.95, 0.05], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# 训练集\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=False\n",
    ")\n",
    "\n",
    "# 验证集\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=False\n",
    ")\n",
    "\n",
    "# 测试集\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    XunFeiDataset(\n",
    "        test_path,\n",
    "        [0] * len(test_path),\n",
    "        A.Compose(\n",
    "            [\n",
    "                A.Resize(256, 256),\n",
    "                A.RandomCrop(224, 224),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.5),\n",
    "                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=50,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 112, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=cv2.imread(train_path[0])\n",
    "np.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XunFeiNet50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XunFeiNet50, self).__init__()\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        model.fc = nn.Linear(2048, 2)\n",
    "        self.resnet = model\n",
    "    def forward(self, img):\n",
    "        out = self.resnet(img)\n",
    "        return out\n",
    "        \n",
    "class XunFeiNet101(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XunFeiNet101, self).__init__()\n",
    "        model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "        model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        model.fc = nn.Linear(2048, 2)\n",
    "        self.resnet = model\n",
    "    def forward(self, img):\n",
    "        out = self.resnet(img)\n",
    "        return out\n",
    "    \n",
    "class XunFeiNet152(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XunFeiNet152, self).__init__()\n",
    "        model = models.resnet152(weights=models.ResNet152_Weights.DEFAULT)\n",
    "        model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        model.fc = nn.Linear(2048, 2)\n",
    "        self.resnet = model\n",
    "    def forward(self, img):\n",
    "        out = self.resnet(img)\n",
    "        return out\n",
    "    \n",
    "class XunFeiNet34(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XunFeiNet34, self).__init__()\n",
    "        model = models.resnet34()\n",
    "        model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        model.fc = nn.Linear(512, 2)\n",
    "        self.resnet = model\n",
    "    def forward(self, img):\n",
    "        out = self.resnet(img)\n",
    "        return out\n",
    "    \n",
    "class XunFeiNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XunFeiNet18, self).__init__()\n",
    "        model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        model.fc = nn.Linear(512, 2)\n",
    "        self.resnet = model\n",
    "    def forward(self, img):\n",
    "        out = self.resnet(img)\n",
    "        return out\n",
    "    \n",
    "model34 = XunFeiNet34().to(device)\n",
    "model50 = XunFeiNet50().to(device)\n",
    "model101= XunFeiNet101().to(device)\n",
    "model152= XunFeiNet152().to(device)\n",
    "# model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train(train_loader, model, criterion, optimizer):\n",
    "    start = time.time()\n",
    "    start_batch = [start, 0]\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    preds = torch.tensor([])\n",
    "    target_all = torch.tensor([])\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            start_batch[(1+(i + 1) // 100) % 2] = time.time()\n",
    "            print(\n",
    "                \"Train loss\",\n",
    "                loss.item(),\n",
    "                \"t={}s\".format(\n",
    "                    start_batch[(1+(i + 1) // 100) % 2]\n",
    "                    - start_batch[((i + 1) // 100) % 2]\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        preds = torch.cat((preds, output.cpu().argmax(1)))\n",
    "        target_all = torch.cat((target_all, target.cpu()))\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    val_acc = torchmetrics.functional.classification.multiclass_f1_score(\n",
    "        preds, target_all, num_classes=2, average=\"macro\"\n",
    "    )\n",
    "    print(\"t={}s\".format(time.time() - start))\n",
    "    print(\"F1 score\", val_acc)\n",
    "    return train_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# 模型验证\n",
    "def validate(val_loader, model, criterion):\n",
    "    model.eval()\n",
    "    val_acc = 0.0\n",
    "    preds = torch.tensor([])\n",
    "    target_all = torch.tensor([])\n",
    "    val_loss=0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            # val_acc += (output.argmax(1) == target).sum().item()\n",
    "            preds = torch.cat((preds, output.cpu().argmax(1)))\n",
    "            target_all = torch.cat((target_all, target.cpu()))\n",
    "\n",
    "        val_acc = torchmetrics.functional.classification.multiclass_f1_score(\n",
    "            preds, target_all, num_classes=2, average=\"macro\"\n",
    "        )\n",
    "        val_loss += loss.item()\n",
    "    # return val_acc / len(val_loader.dataset)\n",
    "    return val_acc,val_loss/len(val_loader)\n",
    "\n",
    "\n",
    "# 模型预测\n",
    "def predict(test_loader, model):\n",
    "    model.eval()\n",
    "\n",
    "    test_pred = []\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(test_loader):\n",
    "            input = input.to(device)\n",
    "            output = model(input)\n",
    "            test_pred.append(output.data.cpu().numpy())\n",
    "\n",
    "    return np.vstack(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for best model\n",
    "model=model152\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.000005)\n",
    "# model=model101\n",
    "epochs = 0\n",
    "last_acc = 0\n",
    "val_acc = 0.001\n",
    "while last_acc < val_acc:\n",
    "    last_acc = val_acc\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    # train_acc = validate(train_loader, model)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epochs + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )\n",
    "    epochs += 1\n",
    "    if val_acc>last_acc:\n",
    "        torch.save(model.state_dict(),\"./model/baseline_152.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.59358149766922 t=37.89586925506592s\n",
      "Train loss 0.5492005944252014 t=65.1143262386322s\n",
      "Train loss 0.5439359545707703 t=89.79940986633301s\n",
      "Train loss 0.498115599155426 t=114.49705696105957s\n",
      "Train loss 0.5084092020988464 t=139.29148244857788s\n",
      "Train loss 0.535024881362915 t=164.13810873031616s\n",
      "Train loss 0.4228997528553009 t=189.23814034461975s\n",
      "Train loss 0.39884406328201294 t=215.17427492141724s\n",
      "t=233.4092059135437s\n",
      "F1 score tensor(0.4689)\n",
      "epoch 1 : train_loss: 0.5194392675577209 val_loss: 0.10963937044143676 val_f1: tensor(0.4504)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ./model does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m epochs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[39mif\u001b[39;00m val_acc\u001b[39m>\u001b[39mlast_acc:\n\u001b[1;32m---> 18\u001b[0m     torch\u001b[39m.\u001b[39;49msave(model\u001b[39m.\u001b[39;49mstate_dict(),\u001b[39m\"\u001b[39;49m\u001b[39m./model/baseline_101.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32md:\\programs\\Anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 440\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32md:\\programs\\Anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[1;32md:\\programs\\Anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mstr\u001b[39;49m(name)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory ./model does not exist."
     ]
    }
   ],
   "source": [
    "# for best model\n",
    "model=model101\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.000005)\n",
    "# model=model101\n",
    "epochs = 0\n",
    "last_acc = 0\n",
    "val_acc = 0.001\n",
    "while last_acc < val_acc:\n",
    "    last_acc = val_acc\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    # train_acc = validate(train_loader, model)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epochs + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )\n",
    "    epochs += 1\n",
    "    if val_acc>last_acc:\n",
    "        torch.save(model.state_dict(),\"./model/baseline_101.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.5575281381607056 t=4.891497373580933s\n",
      "Train loss 0.5705251097679138 t=10.128498792648315s\n",
      "Train loss 0.5479844212532043 t=15.585680484771729s\n",
      "Train loss 0.4515620172023773 t=21.141336679458618s\n",
      "Train loss 0.5779539942741394 t=26.68748378753662s\n",
      "Train loss 0.44088584184646606 t=32.26030945777893s\n",
      "Train loss 0.4428062438964844 t=37.89376711845398s\n",
      "Train loss 0.5259270071983337 t=43.387813329696655s\n",
      "t=46.95921230316162s\n",
      "F1 score tensor(0.4660)\n",
      "epoch 1 : train_loss: 0.5123963730279789 val_loss: 0.10738210678100586 val_f1: tensor(0.4891)\n",
      "Train loss 0.3900531828403473 t=5.018091440200806s\n",
      "Train loss 0.45199495553970337 t=10.624507427215576s\n",
      "Train loss 0.4041074514389038 t=16.265429496765137s\n",
      "Train loss 0.3333797752857208 t=21.955791473388672s\n",
      "Train loss 0.45651888847351074 t=27.641228199005127s\n",
      "Train loss 0.5289812088012695 t=33.328871965408325s\n",
      "Train loss 0.5401110649108887 t=39.11371374130249s\n",
      "Train loss 0.3294830322265625 t=44.81885123252869s\n",
      "t=48.449108600616455s\n",
      "F1 score tensor(0.4615)\n",
      "epoch 2 : train_loss: 0.4348428530055423 val_loss: 0.09449902772903443 val_f1: tensor(0.4891)\n"
     ]
    }
   ],
   "source": [
    "# for best model\n",
    "model=model50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.000005)\n",
    "# model=model101\n",
    "epochs = 0\n",
    "last_acc = 0\n",
    "val_acc = 0.001\n",
    "while last_acc < val_acc:\n",
    "    last_acc = val_acc\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    # train_acc = validate(train_loader, model)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epochs + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )\n",
    "    epochs += 1\n",
    "    if val_acc>last_acc:\n",
    "        torch.save(model.state_dict(),\"./model/baseline_50.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for best model\n",
    "model=model34\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.000005)\n",
    "# model=model101\n",
    "epochs = 0\n",
    "last_acc = 0\n",
    "val_acc = 0.001\n",
    "while last_acc < val_acc:\n",
    "    last_acc = val_acc\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    # train_acc = validate(train_loader, model)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epochs + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )\n",
    "    epochs += 1\n",
    "    if val_acc>last_acc:\n",
    "        torch.save(model.state_dict(),\"./model/baseline_34.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.3254185616970062 t=5.227536916732788s\n",
      "Train loss 0.38939276337623596 t=10.747638940811157s\n",
      "Train loss 0.47319045662879944 t=16.265472888946533s\n",
      "Train loss 0.3093191683292389 t=21.72244358062744s\n",
      "Train loss 0.2806541919708252 t=27.229488134384155s\n",
      "Train loss 0.2751478850841522 t=32.74290728569031s\n",
      "Train loss 0.3427433967590332 t=38.25426983833313s\n",
      "Train loss 0.3619307279586792 t=43.7697811126709s\n",
      "t=47.32741022109985s\n",
      "F1 score tensor(0.4684)\n",
      "epoch 1 : train_loss: 0.3934807222942973 val_loss: 0.08376458287239075 val_f1: tensor(0.4891)\n",
      "Train loss 0.4363410174846649 t=4.991467714309692s\n",
      "Train loss 0.4505930244922638 t=10.55420207977295s\n",
      "Train loss 0.3801509737968445 t=16.1241352558136s\n",
      "Train loss 0.42548298835754395 t=21.758843421936035s\n",
      "Train loss 0.3100317120552063 t=27.410313606262207s\n",
      "Train loss 0.3124871551990509 t=33.33676624298096s\n",
      "Train loss 0.35863491892814636 t=39.17458248138428s\n",
      "Train loss 0.3848431408405304 t=44.79841089248657s\n",
      "t=48.39830183982849s\n",
      "F1 score tensor(0.4876)\n",
      "epoch 2 : train_loss: 0.36500188985536264 val_loss: 0.09618962407112122 val_f1: tensor(0.4891)\n",
      "Train loss 0.2535845637321472 t=4.913836479187012s\n",
      "Train loss 0.40869835019111633 t=10.45949673652649s\n",
      "Train loss 0.1955617368221283 t=16.20512104034424s\n",
      "Train loss 0.33048292994499207 t=21.880960702896118s\n",
      "Train loss 0.3809807598590851 t=27.367557525634766s\n",
      "Train loss 0.2286398857831955 t=32.91071081161499s\n",
      "Train loss 0.45399442315101624 t=38.412044525146484s\n",
      "Train loss 0.4024049639701843 t=43.947590351104736s\n",
      "t=47.4194610118866s\n",
      "F1 score tensor(0.5190)\n",
      "epoch 3 : train_loss: 0.3532679763644241 val_loss: 0.0878701627254486 val_f1: tensor(0.5253)\n",
      "Train loss 0.35132744908332825 t=4.9536426067352295s\n",
      "Train loss 0.30459460616111755 t=10.411680936813354s\n",
      "Train loss 0.3091886341571808 t=15.930270195007324s\n",
      "Train loss 0.36387720704078674 t=21.38479256629944s\n",
      "Train loss 0.23108965158462524 t=26.817140102386475s\n",
      "Train loss 0.25454187393188477 t=32.31810450553894s\n",
      "Train loss 0.247389018535614 t=37.794888734817505s\n",
      "Train loss 0.28629547357559204 t=43.2311429977417s\n",
      "t=46.789323806762695s\n",
      "F1 score tensor(0.5355)\n",
      "epoch 4 : train_loss: 0.3342494846776474 val_loss: 0.08999114036560059 val_f1: tensor(0.4891)\n",
      "Train loss 0.26782408356666565 t=4.890652894973755s\n",
      "Train loss 0.346581369638443 t=10.363188028335571s\n",
      "Train loss 0.2998490631580353 t=15.840921640396118s\n",
      "Train loss 0.38652169704437256 t=21.308093309402466s\n",
      "Train loss 0.2269791066646576 t=26.797345638275146s\n",
      "Train loss 0.35793840885162354 t=32.29916501045227s\n",
      "Train loss 0.3528255224227905 t=37.80574083328247s\n",
      "Train loss 0.3298017084598541 t=43.2221143245697s\n",
      "t=46.72625517845154s\n",
      "F1 score tensor(0.5388)\n",
      "epoch 5 : train_loss: 0.321378456818503 val_loss: 0.09474175572395324 val_f1: tensor(0.5253)\n",
      "Train loss 0.2689189910888672 t=4.9212095737457275s\n",
      "Train loss 0.22748538851737976 t=10.421052932739258s\n",
      "Train loss 0.2874981462955475 t=16.0548677444458s\n",
      "Train loss 0.3200199007987976 t=21.77425456047058s\n",
      "Train loss 0.34880638122558594 t=27.387758016586304s\n",
      "Train loss 0.29564014077186584 t=32.98695492744446s\n",
      "Train loss 0.37468159198760986 t=38.48081111907959s\n",
      "Train loss 0.3337850570678711 t=43.96970868110657s\n",
      "t=47.53549408912659s\n",
      "F1 score tensor(0.5943)\n",
      "epoch 6 : train_loss: 0.30578362421933997 val_loss: 0.08973711133003234 val_f1: tensor(0.5843)\n",
      "Train loss 0.29476940631866455 t=5.032759666442871s\n",
      "Train loss 0.21089549362659454 t=10.510054111480713s\n",
      "Train loss 0.21055874228477478 t=16.209790468215942s\n",
      "Train loss 0.3547869324684143 t=22.09659218788147s\n",
      "Train loss 0.23680636286735535 t=27.704235076904297s\n",
      "Train loss 0.16558994352817535 t=33.32168006896973s\n",
      "Train loss 0.36913013458251953 t=38.7830011844635s\n",
      "Train loss 0.2887979745864868 t=44.242469787597656s\n",
      "t=47.74741196632385s\n",
      "F1 score tensor(0.6180)\n",
      "epoch 7 : train_loss: 0.3028099571549615 val_loss: 0.06497641205787659 val_f1: tensor(0.7143)\n",
      "Train loss 0.2561153769493103 t=4.954282522201538s\n",
      "Train loss 0.34476879239082336 t=10.48068380355835s\n",
      "Train loss 0.2801695466041565 t=15.956828117370605s\n",
      "Train loss 0.2421441227197647 t=21.400598287582397s\n",
      "Train loss 0.3131425976753235 t=26.830864906311035s\n",
      "Train loss 0.22867806255817413 t=32.29339098930359s\n",
      "Train loss 0.23876023292541504 t=37.83772611618042s\n",
      "Train loss 0.41733449697494507 t=43.42164731025696s\n",
      "t=46.98145270347595s\n",
      "F1 score tensor(0.6220)\n",
      "epoch 8 : train_loss: 0.2943679823778396 val_loss: 0.07831161618232726 val_f1: tensor(0.6968)\n",
      "Train loss 0.4324054718017578 t=4.92990255355835s\n",
      "Train loss 0.1791616976261139 t=10.390138626098633s\n",
      "Train loss 0.2055155336856842 t=15.904208660125732s\n",
      "Train loss 0.2588103711605072 t=21.388670682907104s\n",
      "Train loss 0.26173558831214905 t=26.823954820632935s\n",
      "Train loss 0.20560549199581146 t=32.26537203788757s\n",
      "Train loss 0.3296032249927521 t=37.74299645423889s\n",
      "Train loss 0.31356820464134216 t=43.214653968811035s\n",
      "t=46.76044940948486s\n",
      "F1 score tensor(0.6572)\n",
      "epoch 9 : train_loss: 0.2831190228462219 val_loss: 0.06967198252677917 val_f1: tensor(0.7770)\n",
      "Train loss 0.3508616089820862 t=4.920201301574707s\n",
      "Train loss 0.22098830342292786 t=10.569837808609009s\n",
      "Train loss 0.20408786833286285 t=16.093254327774048s\n",
      "Train loss 0.27047500014305115 t=21.619097471237183s\n",
      "Train loss 0.3614649474620819 t=27.138102531433105s\n",
      "Train loss 0.186218723654747 t=32.72275996208191s\n",
      "Train loss 0.20658278465270996 t=38.19761061668396s\n",
      "Train loss 0.288865864276886 t=43.79434418678284s\n",
      "t=47.29841589927673s\n",
      "F1 score tensor(0.6924)\n",
      "epoch 10 : train_loss: 0.2738632088483766 val_loss: 0.07401833534240723 val_f1: tensor(0.7213)\n"
     ]
    }
   ],
   "source": [
    "model=model50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.000005)\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epoch + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对测试集多次预测\n",
    "pred = None\n",
    "model=model101\n",
    "model.load_state_dict(torch.load(\"./model/baseline_101.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += predict(test_loader, model)\n",
    "    print(_+1)\n",
    "model=model152\n",
    "model.load_state_dict(torch.load(\"./model/baseline_152.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += 0.9*predict(test_loader, model)\n",
    "    print(_+1)\n",
    "model=model50\n",
    "model.load_state_dict(torch.load(\"./model/baseline_50.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += predict(test_loader, model)\n",
    "    print(_+1)\n",
    "model=model34\n",
    "model.load_state_dict(torch.load(\"./model/baseline_34.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += 0.9*predict(test_loader, model)\n",
    "    print(_+1)\n",
    "submit = pd.DataFrame(\n",
    "    {\n",
    "        'name': [x.split('/')[-1] for x in test_path],\n",
    "        'label': pred.argmax(1)\n",
    "})\n",
    "\n",
    "# 生成提交结果\n",
    "submit = submit.sort_values(by='name')\n",
    "submit.to_csv('submit6.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_loss = 0.0\n",
    "model = model.to(device)\n",
    "for i, (input, target) in enumerate(train_loader):\n",
    "    input = input.to(device)\n",
    "    target = target.to(device)\n",
    "    output = model(input)\n",
    "    loss = criterion(output, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=4\n",
    "model=model101\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.0001)\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epoch + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )\n",
    "    epochs += 1\n",
    "torch.save(model.state_dict(),\"./model/baseline_test.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=3\n",
    "model101= XunFeiNet().to(device)\n",
    "model=model101\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.0001)\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "    print(\n",
    "        \"epoch {} :\".format(epoch + 1), \"train_loss:\", train_loss, 'val_loss:', val_loss,\"val_f1:\", val_acc\n",
    "    )\n",
    "    epochs += 1\n",
    "torch.save(model.state_dict(),\"./model/baseline_101.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = None\n",
    "model=model101\n",
    "model.load_state_dict(torch.load(\"./model/baseline_test.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += predict(test_loader, model)\n",
    "    print(_+1)\n",
    "model.load_state_dict(torch.load(\"./model/baseline_101.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += predict(test_loader, model)\n",
    "    print(_+1)\n",
    "model=model50\n",
    "model.load_state_dict(torch.load(\"./model/baseline_50.pth\"))\n",
    "for _ in range(3):\n",
    "    if pred is None:\n",
    "        pred = predict(test_loader, model)\n",
    "    else:\n",
    "        pred += predict(test_loader, model)\n",
    "    print(_+1)\n",
    "submit = pd.DataFrame(\n",
    "    {\n",
    "        'name': [x.split('/')[-1] for x in test_path],\n",
    "        'label': pred.argmax(1)\n",
    "})\n",
    "\n",
    "# 生成提交结果\n",
    "submit = submit.sort_values(by='name')\n",
    "submit.to_csv('submit5.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = validate(val_loader, model)\n",
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=XunFeiDataset(train_path[:-1000], train_label[:-1000],\n",
    "            A.Compose([\n",
    "            # A.RandomRotate90(),\n",
    "            A.Resize(256, 256),\n",
    "            A.RandomCrop(224, 224),\n",
    "            # A.HorizontalFlip(p=0.5),\n",
    "            # A.RandomBrightnessContrast(p=0.5),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        )\n",
    "train_dataset, test_dataset=torch.utils.data.random_split(dataset=dataset,lengths=[0.9,0.1],generator=torch.Generator().manual_seed(42))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./model/baseline_test.pth\"))\n",
    "val_acc,val_loss = validate(val_loader, model, criterion)\n",
    "print(val_loss)\n",
    "len(val_loader.dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
